{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOS/F8GyM4AyUqz1gyYWMLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tleyden/FruitPunch_AI_Bootcamp/blob/main/FruitPunch_Capstone_Project_YOLO_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip installs"
      ],
      "metadata": {
        "id": "Y_6rPZmkpH_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y jq\n",
        "!pip install pycocotools\n",
        "!python -m pip install pyyaml==5.1\n",
        "!pip install wandb -qU\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lVa3C0lpLSA",
        "outputId": "c4444786-284a-45ef-9758-fd58f98aba2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libjq1 libonig4\n",
            "The following NEW packages will be installed:\n",
            "  jq libjq1 libonig4\n",
            "0 upgraded, 3 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 276 kB of archives.\n",
            "After this operation, 930 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\n",
            "Fetched 276 kB in 0s (829 kB/s)\n",
            "Selecting previously unselected package libonig4:amd64.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\n",
            "Unpacking libonig4:amd64 (6.7.0-1) ...\n",
            "Selecting previously unselected package libjq1:amd64.\n",
            "Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Selecting previously unselected package jq.\n",
            "Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking jq (1.5+dfsg-2) ...\n",
            "Setting up libonig4:amd64 (6.7.0-1) ...\n",
            "Setting up libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Setting up jq (1.5+dfsg-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (2.0.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pycocotools) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 4.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp38-cp38-linux_x86_64.whl size=44089 sha256=dac58e85c1fbcaa81db6a06c4f60e36a677b099c8b7cfbae92aea60d16c30cc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/dd/2b/10ff8b0ac81b93946bb5fb9e6749bae2dac246506c8774e6cf\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2022.2.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed pyyaml-5.1\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 173 kB 37.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 65.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 81.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 86.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 85.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 88.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 83.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 85.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 88.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 82.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 84.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 82.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 78.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 84.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 86.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 84.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 74.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "51m3p5WRsjSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "import skimage.io as io\n",
        "from pathlib import Path\n",
        "\n",
        "import sys, distutils.core\n",
        "from google.colab.patches import cv2_imshow\n",
        "import json\n",
        "import shutil\n",
        "import yaml"
      ],
      "metadata": {
        "id": "B_V4uM35si1c"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights and biases setup"
      ],
      "metadata": {
        "id": "kdJ3uuWVO5sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "w7xr4jTKO8Y5",
        "outputId": "188a9d06-4a51-4449-ab0c-f4a6bfd0be36"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount google drive"
      ],
      "metadata": {
        "id": "UPxqYyCGSD3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4crRMlDCSFeX",
        "outputId": "3628b924-e970-4dde-d1e1-6fb5d335d81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download zips from google bucket"
      ],
      "metadata": {
        "id": "UyXcuhJHr02I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "if not os.path.exists(\"Labeled data-20211126T095740Z-001.zip\"):\n",
        "    !gsutil cp \"gs://fruitpunch-ai-tleyden/Labeled data-20211126T095740Z-001.zip\" .\n",
        "if not os.path.exists(\"Labeled data-20211126T095740Z-002.zip\"):\n",
        "    !gsutil cp \"gs://fruitpunch-ai-tleyden/Labeled data-20211126T095740Z-002.zip\" .\n"
      ],
      "metadata": {
        "id": "Zax75VsOr5Ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cc8b50-76f1-44d6-9939-5c9da532cdae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://fruitpunch-ai-tleyden/Labeled data-20211126T095740Z-001.zip...\n",
            "| [1 files][  2.0 GiB/  2.0 GiB]   79.9 MiB/s                                   \n",
            "Operation completed over 1 objects/2.0 GiB.                                      \n",
            "Copying gs://fruitpunch-ai-tleyden/Labeled data-20211126T095740Z-002.zip...\n",
            "/ [1 files][809.6 MiB/809.6 MiB]   83.7 MiB/s                                   \n",
            "Operation completed over 1 objects/809.6 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"labeled_data\"):\n",
        "    !unzip -q \"Labeled data-20211126T095740Z-001.zip\"\n",
        "    !unzip -q \"Labeled data-20211126T095740Z-002.zip\""
      ],
      "metadata": {
        "id": "4QT3o_GrsTJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-arrange directories to match expected structure"
      ],
      "metadata": {
        "id": "pfZpqkXBvnDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"labeled_data\"):\n",
        "    os.makedirs(\"labeled_data\")\n",
        "    !mv \"Labeled data\" labeled_data\n",
        "if not os.path.exists(\"labeled_data/images/test\"):\n",
        "    !mkdir labeled_data/images/test \n",
        "    !mv labeled_data/images/*.PNG labeled_data/images/test "
      ],
      "metadata": {
        "id": "rJa_ntnutA2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47817f48-d5c9-4022-e342-ea184339acf8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already have labeled_data dir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set some global variables"
      ],
      "metadata": {
        "id": "1h5iqPpTv1_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/labeled_data/'\n",
        "\n",
        "\n",
        "use_single_batch = False\n",
        "if use_single_batch:\n",
        "    # Smaller subset of dataset for faster iteration\n",
        "    LABELS_PATH = DATA_PATH + 'annotations_single_batch/'\n",
        "    IMAGES_PATH = DATA_PATH + 'images_single_batch/'\n",
        "else:\n",
        "    LABELS_PATH = DATA_PATH + 'annotations/'\n",
        "    IMAGES_PATH = DATA_PATH + 'images/'\n",
        "\n",
        "# Get paths to IMAGE directories\n",
        "TRAIN_IMAGES_PATH = IMAGES_PATH + 'train/'\n",
        "TEST_IMAGES_PATH = IMAGES_PATH + 'test/'\n",
        "VAL_IMAGES_PATH = IMAGES_PATH + 'val/'\n",
        "\n",
        "TRAIN_LABELS = LABELS_PATH + 'instances_train.json'\n",
        "TEST_LABELS = LABELS_PATH + 'instances_test_dataset.json'\n",
        "VAL_LABELS = LABELS_PATH + 'instances_val.json'\n",
        "\n"
      ],
      "metadata": {
        "id": "YZKCCXxgvZFN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create single batch annotations\n",
        "\n",
        "Create a smaller dataset that can be used for faster iteration"
      ],
      "metadata": {
        "id": "QFinEDQXWKn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_single_batch_annotations(num_instances_to_collect):\n",
        "    \"\"\"\n",
        "    Create a copy of annotations/instances_train.json, instances_test_dataset.json and instances_val.json\n",
        "    in the annotations_single_batch directory with a smaller subset of annotations.\n",
        "    \"\"\"\n",
        "    source_annotations = [\"instances_train.json\", \"instances_test_dataset.json\", \"instances_val.json\"]\n",
        "    for source_annotation in source_annotations:\n",
        "        subset_json = {}\n",
        "        with open(os.path.join(DATA_PATH, \"annotations\", source_annotation), \"r\") as f:\n",
        "            instances_json = json.loads(f.read())\n",
        "        subset_json[\"licenses\"] = instances_json[\"licenses\"]\n",
        "        subset_json[\"info\"] = instances_json[\"info\"]\n",
        "        subset_json[\"categories\"] = instances_json[\"categories\"]\n",
        "        subset_images = []\n",
        "        subset_annotations = []\n",
        "        collected_image_ids = []\n",
        "        \n",
        "        for annotation_json in instances_json[\"annotations\"]:\n",
        "            if len(subset_annotations) >= num_instances_to_collect:\n",
        "                # We have collected enough\n",
        "                break\n",
        "            subset_annotations.append(annotation_json)\n",
        "            collected_image_ids.append(annotation_json[\"image_id\"])\n",
        "            \n",
        "        for image_json in instances_json[\"images\"]:\n",
        "            if image_json[\"id\"] not in collected_image_ids:\n",
        "                continue\n",
        "            subset_images.append(image_json)\n",
        "\n",
        "            # Copy image to target image dir\n",
        "            if \"test\" in source_annotation:\n",
        "                target_img_dir = \"test\"\n",
        "            elif \"train\" in source_annotation:\n",
        "                target_img_dir = \"train\"\n",
        "            elif \"val\" in source_annotation:\n",
        "                target_img_dir = \"val\"\n",
        "\n",
        "            file_name = image_json[\"file_name\"]\n",
        "            source_img = os.path.join(DATA_PATH, \"images\", target_img_dir, file_name)\n",
        "            target_img = os.path.join(DATA_PATH, \"images_single_batch\", target_img_dir, file_name)\n",
        "            if not os.path.exists(target_img):\n",
        "                os.link(source_img, target_img)\n",
        "\n",
        "\n",
        "        print(f\"For {source_annotation} collected {len(subset_annotations)} instances\")\n",
        "        subset_json[\"images\"] = subset_images\n",
        "        subset_json[\"annotations\"] = subset_annotations\n",
        "\n",
        "        # Write json to target dir\n",
        "        source_annotation_full_path = os.path.join(DATA_PATH, \"annotations_single_batch\", source_annotation)\n",
        "        with open(source_annotation_full_path, \"w\") as f:\n",
        "            json.dump(subset_json, f)\n",
        "        print(f\"Wrote {source_annotation_full_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "44MZIXAAWRlu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_single_batch:\n",
        "    if not os.path.exists(os.path.join(DATA_PATH, \"annotations_single_batch\")):\n",
        "        os.makedirs(os.path.join(DATA_PATH, \"annotations_single_batch\"))\n",
        "    if not os.path.exists(os.path.join(DATA_PATH, \"images_single_batch\")):\n",
        "        os.makedirs(os.path.join(DATA_PATH, \"images_single_batch\"))\n",
        "        os.makedirs(os.path.join(DATA_PATH, \"images_single_batch\", \"train\"))\n",
        "        os.makedirs(os.path.join(DATA_PATH, \"images_single_batch\", \"test\"))\n",
        "        os.makedirs(os.path.join(DATA_PATH, \"images_single_batch\", \"val\"))\n",
        "\n",
        "create_single_batch_annotations(64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msxKl_wvIxcf",
        "outputId": "24176dfa-dbfe-413a-9f47-9df3d93fe91b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For instances_train.json collected 64 instances\n",
            "Wrote /content/labeled_data/annotations_single_batch/instances_train.json\n",
            "For instances_test_dataset.json collected 64 instances\n",
            "Wrote /content/labeled_data/annotations_single_batch/instances_test_dataset.json\n",
            "For instances_val.json collected 64 instances\n",
            "Wrote /content/labeled_data/annotations_single_batch/instances_val.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert annotations from coco -> yolo format"
      ],
      "metadata": {
        "id": "IRb7NnkpGzY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/JSON2YOLO.git  # clone repo\n",
        "%pip install -qr JSON2YOLO/requirements.txt # install dependencies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1Psxob0G41J",
        "outputId": "83a12d43-1fd9-4dc6-c4c2-f95c39ff2289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'JSON2YOLO'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 277 (delta 102), reused 84 (delta 82), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (277/277), 74.65 KiB | 1.96 MiB/s, done.\n",
            "Resolving deltas: 100% (177/177), done.\n",
            "/content/JSON2YOLO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from general_json2yolo import convert_coco_json\n",
        "\n",
        "if use_single_batch:\n",
        "    convert_coco_json('/content/labeled_data/annotations_single_batch')\n",
        "else:\n",
        "    convert_coco_json('/content/labeled_data/annotations')"
      ],
      "metadata": {
        "id": "BJBH8kLjG9-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5edccd-8981-457f-87ba-c34caf7976b3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Annotations /content/labeled_data/annotations/instances_test_dataset.json: 100%|██████████| 1900/1900 [00:00<00:00, 4920.98it/s]\n",
            "Annotations /content/labeled_data/annotations/instances_train.json: 100%|██████████| 13111/13111 [00:01<00:00, 7081.87it/s]\n",
            "Annotations /content/labeled_data/annotations/instances_val.json: 100%|██████████| 3278/3278 [00:00<00:00, 7147.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy images to yolo dir\n",
        "\n",
        "One confusing thing here is that it actually puts the images into the labels directory to keep them in a single directory (as expected by yolov5).  A TODO item is to rename this directory to be clearer"
      ],
      "metadata": {
        "id": "Y1DWJ-BPJh78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_single_batch:\n",
        "    imgs_source_dir = os.path.join(\"labeled_data\", \"images_single_batch\")\n",
        "else:\n",
        "    imgs_source_dir = os.path.join(\"labeled_data\", \"images\")\n",
        "\n",
        "imgs_train = os.path.join(imgs_source_dir, \"train\")\n",
        "imgs_test = os.path.join(imgs_source_dir, \"test\")\n",
        "imgs_val = os.path.join(imgs_source_dir, \"val\")\n",
        "\n",
        "imgs_target_dir = os.path.join(\"new_dir\", \"labels\")\n",
        "imgs_target_train =  os.path.join(imgs_target_dir, \"train\")\n",
        "imgs_target_test =  os.path.join(imgs_target_dir, \"test_dataset\")\n",
        "imgs_target_val =  os.path.join(imgs_target_dir, \"val\")\n",
        "\n",
        "!cp {imgs_train}/*.PNG {imgs_target_train}/\n",
        "!cp {imgs_test}/*.PNG {imgs_target_test}/\n",
        "!cp {imgs_val}/*.PNG {imgs_target_val}/\n"
      ],
      "metadata": {
        "id": "CBBELtKZI97_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create yolo data yaml file"
      ],
      "metadata": {
        "id": "LRmq8YRiMHJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_data_yaml = {\n",
        "    \"nc\": 1,\n",
        "    \"names\": [\"Human\"],\n",
        "    \"train\": [\"/content/new_dir/labels/train\"],\n",
        "    \"test\": [\"/content/new_dir/labels/test_dataset\"],\n",
        "    \"val\": [\"/content/new_dir/labels/val\"]\n",
        "}\n",
        "\n",
        "with open('data.yaml', 'w') as file:\n",
        "    yaml.dump(yolo_data_yaml, file)"
      ],
      "metadata": {
        "id": "I3JkAPXdMIwY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYHz9ks2Nujd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install yolo"
      ],
      "metadata": {
        "id": "UH5X22BfOWKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"yolov5\"):\n",
        "    !git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "    %pip install -qr yolov5/requirements.txt # install dependencies\n"
      ],
      "metadata": {
        "id": "tNVy0GuqOXsa"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train yolo"
      ],
      "metadata": {
        "id": "Ql91i27LOJ_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolov5/train.py --img 640 --batch 24 --epochs 300 --data data.yaml --weights yolov5l.pt --cache\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x32hDrtOLXA",
        "outputId": "a2902b97-213b-437c-e4fd-69f719cfb595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtleyden\u001b[0m (\u001b[33meyepi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=, data=data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=300, batch_size=24, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-34-g1ae9194 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20221215_221429-1o78txqy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmart-blaze-199\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/eyepi/train\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/eyepi/train/runs/1o78txqy\u001b[0m\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
            "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
            "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
            "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
            " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
            " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
            " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
            " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
            " 24      [17, 20, 23]  1     32310  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model summary: 368 layers, 46138294 parameters, 46138294 gradients, 108.2 GFLOPs\n",
            "\n",
            "Transferred 607/613 items from yolov5l.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 101 weight(decay=0.0), 104 weight(decay=0.0005625000000000001), 104 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/new_dir/labels/train.cache... 13111 images, 0 backgrounds, 0 corrupt: 100% 13111/13111 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m11.0GB RAM required, 9.5/12.7GB available, not caching images ⚠️\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/new_dir/labels/val.cache... 3278 images, 0 backgrounds, 0 corrupt: 100% 3278/3278 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.8GB ram): 100% 3278/3278 [00:14<00:00, 229.14it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.73 anchors/target, 0.996 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to yolov5/runs/train/exp10/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolov5/runs/train/exp10\u001b[0m\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      0/299      14.4G    0.08414    0.01768          0         20        640: 100% 547/547 [08:30<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.52it/s]\n",
            "                   all       3278       8417      0.527      0.552      0.482      0.167\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      1/299      13.6G    0.07064    0.01213          0         39        640: 100% 547/547 [08:12<00:00,  1.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.51it/s]\n",
            "                   all       3278       8417      0.665      0.623      0.629      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      2/299      13.6G    0.06982    0.01173          0         27        640: 100% 547/547 [08:10<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.53it/s]\n",
            "                   all       3278       8417      0.653      0.654      0.669      0.276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      3/299      13.6G    0.06646    0.01145          0         42        640: 100% 547/547 [08:07<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.52it/s]\n",
            "                   all       3278       8417      0.689      0.676      0.701      0.318\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      4/299      13.6G    0.06343     0.0112          0         32        640: 100% 547/547 [08:09<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:44<00:00,  1.55it/s]\n",
            "                   all       3278       8417      0.712      0.666      0.703      0.299\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      5/299      13.6G    0.06331    0.01108          0         24        640: 100% 547/547 [08:04<00:00,  1.13it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.52it/s]\n",
            "                   all       3278       8417      0.709      0.673      0.719      0.335\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      6/299      13.6G    0.06249    0.01102          0         21        640: 100% 547/547 [08:09<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.52it/s]\n",
            "                   all       3278       8417      0.727      0.678      0.722      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      7/299      13.6G    0.06089    0.01064          0         20        640: 100% 547/547 [08:06<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.53it/s]\n",
            "                   all       3278       8417      0.717      0.705      0.731      0.326\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      8/299      13.6G    0.06018    0.01076          0         30        640: 100% 547/547 [08:05<00:00,  1.13it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.52it/s]\n",
            "                   all       3278       8417      0.741      0.698      0.748      0.363\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      9/299      13.6G    0.05989    0.01068          0         30        640: 100% 547/547 [08:06<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.53it/s]\n",
            "                   all       3278       8417      0.752      0.716      0.764      0.373\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     10/299      13.6G     0.0594    0.01071          0         38        640: 100% 547/547 [08:09<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:45<00:00,  1.52it/s]\n",
            "                   all       3278       8417      0.699      0.685      0.721      0.348\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     11/299      13.6G    0.05921    0.01068          0         22        640: 100% 547/547 [08:06<00:00,  1.13it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [00:44<00:00,  1.54it/s]\n",
            "                   all       3278       8417      0.731      0.707      0.756      0.377\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     12/299      13.6G    0.06114     0.0108          0         87        640:  44% 238/547 [03:32<04:35,  1.12it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval yolo"
      ],
      "metadata": {
        "id": "kBmhd_bhL782"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate bbox predictions on test set"
      ],
      "metadata": {
        "id": "TfFlE8GIMAzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolov5/detect.py --device 0 --weights traun_fruitpunch_poachers_best.pt --source /content/new_dir/labels/test_dataset\n"
      ],
      "metadata": {
        "id": "C8j7PBL0L_ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate mAP score on test set"
      ],
      "metadata": {
        "id": "Wwx0Ze2nMK2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolov5/val.py --device 0 --weights traun_fruitpunch_poachers_best.pt --data data.yaml"
      ],
      "metadata": {
        "id": "5COg86TAMRne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display a few test predictions next to actual labels\n"
      ],
      "metadata": {
        "id": "I6u9P_lUMVwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def superimpose_yolo_boxes(image_path, yolo_labelfile_path, classnames):\n",
        "    \"\"\"\n",
        "    Return an image object with the yolo bounding boxes superimposed.\n",
        "    \n",
        "    :param classnames: a list of classnames.  The yolo labels will have the index, \n",
        "                       and indexing into classnames will give the actual classname \n",
        "                       (eg, \"forklift\")\n",
        "    \n",
        "    \n",
        "    Steps:\n",
        "    \n",
        "    1. Load the image from the image path\n",
        "    2. Load the yolo labels\n",
        "    3. For each yolo label, draw a bounding box\n",
        "    4. Return the image\n",
        "    \"\"\"\n",
        "    \n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, channels = image.shape\n",
        "    line_thickness = round(0.002 * (image.shape[0] + image.shape[1]) / 2) + 1 \n",
        "    \n",
        "    with open(yolo_labelfile_path, \"r\") as yolo_labelfile:\n",
        "        for line in yolo_labelfile:\n",
        "            fields = line.split()\n",
        "            class_label = classnames[int(fields[0])]\n",
        "            \n",
        "            normalized_x_center = float(fields[1])\n",
        "            normalized_y_center = float(fields[2])\n",
        "            normalized_w = float(fields[3])\n",
        "            normalized_h = float(fields[4])\n",
        "            \n",
        "            x_center = normalized_x_center*width\n",
        "            y_center = normalized_y_center*height\n",
        "            w = normalized_w*width\n",
        "            h = normalized_h*height                        \n",
        "            \n",
        "            xmin = round(x_center - w/2)  # Left\n",
        "            ymin = round(y_center - h/2)  # Top\n",
        "            xmax = round(x_center + w/2)  # Right\n",
        "            ymax = round(y_center + h/2)  # Bottom  \n",
        "                        \n",
        "            color = [random.randint(0, 255) for _ in range(3)]\n",
        "\n",
        "            top_left = (xmin, ymin)\n",
        "            bottom_right = (xmax, ymax)\n",
        "            cv2.rectangle(image, top_left, bottom_right, color, thickness=line_thickness, lineType=cv2.LINE_AA)\n",
        "            \n",
        "            # Shift the label down a few pixels if the bounding box is at the top of the image\n",
        "            top_left_with_offset = top_left\n",
        "            if ymin <= 20:\n",
        "                top_left_with_offset = (xmin, ymin + 20)\n",
        "            cv2.putText(image, str(class_label), top_left_with_offset, 0, line_thickness / 3, [225, 255, 255], thickness=line_thickness, lineType=cv2.LINE_AA)\n",
        "            \n",
        "        RGB_im = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        return RGB_im"
      ],
      "metadata": {
        "id": "ALizfpFNMZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "test_predictions_path = \"yolov5/runs/detect/exp4\"\n",
        "\n",
        "# Get a few sample predicted images\n",
        "all_images = os.listdir(test_predictions_path)\n",
        "sampled_prediction_images = random.sample(all_images, 3)\n",
        "print(sampled_prediction_images)\n",
        "help(random.sample)\n",
        "\n",
        "# Find the corresponding test images (non-superimposed bboxes) and their corresponding labels files\n",
        "test_images_path = os.path.join(CONTENT_ROOT, \"new_dir\", \"labels\", \"test_dataset\")\n",
        "unlabeled_test_images = []\n",
        "label_files_for_test_images = []\n",
        "for img in sampled_prediction_images:\n",
        "    img_full_path = os.path.join(test_images_path, img)\n",
        "    filename_no_extension = os.path.splitext(img)[0]\n",
        "    yolo_label_file_full_path = os.path.join(test_images_path, f\"{filename_no_extension}.txt\")\n",
        "    print(f\"img_full_path: {img_full_path} yolo_label_file_full_path: {yolo_label_file_full_path}\")\n",
        "    unlabeled_test_images.append(img_full_path)\n",
        "    label_files_for_test_images.append(yolo_label_file_full_path)\n",
        "    \n",
        "\n",
        "# Superimpose yolo labels on test images\n",
        "# Plot them side by side with ground truth labeled test images\n",
        "_, axs = plt.subplots(nrows=len(sampled_prediction_images), ncols=2, figsize=(20, 20))\n",
        "for sampled_prediction_image, unlabeled_test_image, label_files_for_test_image, ax in zip(sampled_prediction_images, unlabeled_test_images, label_files_for_test_images, axs):\n",
        "    print(f\"Displaying image: {sampled_prediction_image}, {unlabeled_test_image}, {label_files_for_test_image} on ax: {ax}\")\n",
        "    superimposed_img = superimpose_yolo_boxes(\n",
        "        image_path=unlabeled_test_image, \n",
        "        yolo_labelfile_path=label_files_for_test_image, \n",
        "        classnames=[\"Human\"]\n",
        "    )\n",
        "    # print(f\"superimposed_img: {superimposed_img}\")\n",
        "    sampled_prediction_image_full_path = os.path.join(test_predictions_path, sampled_prediction_image)\n",
        "    sampled_prediction_image_cv2 = cv2.imread(sampled_prediction_image_full_path)\n",
        "    ax[0].imshow(sampled_prediction_image_cv2)\n",
        "    ax[1].imshow(superimposed_img)"
      ],
      "metadata": {
        "id": "4tHW7lOOMgN1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}